services:
  source_postgres:
    image: postgres:15
    ports:
      - "5433:5432"
    networks:
      - elt_network
    environment:
      POSTGRES_DB: source_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
    # how we persist data when stopping/starting containers; reference is like local:/docker
    volumes: 
      - ./source_db_init/init.sql:/docker-entrypoint-initdb.d/init.sql

  destination_postgres:
    image: postgres:15
    ports:
      - "5434:5432"
    networks:
      - elt_network
    environment:
      POSTGRES_DB: destination_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
    # we don't want to persist this data because we won't know if things are working if the data doesn't vanish when we shut this down.

# commenting these two out with the addition of airflow, since it'll be doing this instead
  # elt_script:
  #   # we'll be using docker compose but some things are easier to run as-needed, so we're building this section out
  #   build:
  #     context: ./elt # root, but in reference to the docker file
  #     dockerfile: Dockerfile
  #   command: ['python', 'elt_script.py']
  #   networks:
  #     - elt_network
  #   depends_on:
  #     # won't run until these finish initializing, obviously
  #     source_postgres:
  #       condition: service_started
  #     destination_postgres:
  #       condition: service_started

  # dbt:
  #   image: ghcr.io/dbt-labs/dbt-postgres:1.4.7
  #   command:
  #     [
  #       "run",
  #       "--profiles-dir",
  #       "/root",
  #       "--project-dir",
  #       "/dbt",
  #       "--full-refresh"
  #     ]
  #   networks:
  #     - elt_network
  #   volumes:
  #     - ./custom_postgres:/dbt
  #     - ~/.dbt:/root
  #   depends_on:
  #     elt_script:
  #       condition: service_completed_successfully
  #   environment:
  #     DBT_PROFILE: default
  #     DBT_TARGET: dev

# adding airflow, which needs postgres to do it's thing
  postgres:
    image: postgres:15
    networks:
      - elt_network
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow

# need this to do things like create admin user, create connections, etc
# issues all day everyday with logging into localhost, seems to be an issue with creating users. trying a third time to remedy this,
  init-airflow:
    image: apache/airflow:2.10.2
    container_name: init-airflow
    depends_on:
      - postgres
    networks:
      - elt_network
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=ZsId019_jeppe-J5U0Ig3M7ydQbjgCGsr2ftHq6I-So=
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__WEBSERVER__SECRET_KEY=supersecretkey
    command: >
      bash -c "
        echo 'üì¶ Waiting for Postgres...' &&
        sleep 5 &&
        echo '‚öôÔ∏è Initializing Airflow DB...' &&
        airflow db upgrade &&
        echo '‚öôÔ∏è Running Airflow DB migrations...' &&
        airflow db migrate &&
        echo 'üë§ Creating admin user...' &&
        airflow users create --username airflow --firstname John --lastname Doe --role Admin --email admin@example.com --password password &&
        echo '‚úÖ Airflow initialization complete.'
      "

  # init-airflow:
  #   image: apache/airflow:latest
  #   depends_on:
  #     - postgres
  #   networks:
  #     - elt_network
  #   environment:
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
  #     - AIRFLOW__WEBSERVER__DEFAULT_USER_USERNAME=airflow
  #     - AIRFLOW__WEBSERVER__DEFAULT_USER_PASSWORD=password
  #     - _AIRFLOW_DB_MIGRATE=true
  #     - _AIRFLOW_WWW_USER_CREATE=true
  #     - _AIRFLOW_WWW_USER_USERNAME=airflow
  #     - _AIRFLOW_WWW_USER_PASSWORD=password
  #     - _AIRFLOW_WWW_USER_FIRSTNAME=John
  #     - _AIRFLOW_WWW_USER_LASTNAME=Doe
  #     - _AIRFLOW_WWW_USER_EMAIL=admin@example.com
  #     - _AIRFLOW_WWW_USER_ROLE=Admin
  #   command: ["airflow", "db", "migrate"]

  # getting into the ui of airflow with this. creating a dockerfile to do so.
  webserver:
    build:
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      postgres:
        condition: service_started
      init-airflow:
        condition: service_completed_successfully
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./elt_script:/opt/airflow/elt_script
      - ./postgres_transformations:/opt/dbt
      - ~/.dbt:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW_CONN_DESTINATION_POSTGRES=postgres://postgres:secret@destination_postgres:5434/destination_db
      - AIRFLOW__CORE__FERNET_KEY=ZsId019_jeppe-J5U0Ig3M7ydQbjgCGsr2ftHq6I-So=
      - AIRFLOW__WEBSERVER__DEFAULT_USER_USERNAME=airflow
      - AIRFLOW__WEBSERVER__DEFAULT_USER_PASSWORD=password
      - AIRFLOW_WWW_USER_USERNAME=airflow
      - AIRFLOW_WWW_USER_PASSWORD=password
      - AIRFLOW__API__SECRET_KEY=secret
    ports:
      - "8080:8080"
    command: webserver

  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      postgres:
        condition: service_started
      init-airflow:
        condition: service_completed_successfully
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./elt_script:/opt/airflow/elt_script
      - ./postgres_transformations:/dbt
      - ~/.dbt:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW_CONN_DESTINATION_POSTGRES=postgres://postgres:secret@destination_postgres:5434/destination_db
      - AIRFLOW__CORE__FERNET_KEY=plIipb9RU3-3wJ1UNaAtqVNJrqFEks1-dGbJM34EW7U=
      - AIRFLOW__API__SECRET_KEY=secret
      - AIRFLOW_WWW_USER_USERNAME=airflow
      - AIRFLOW_WWW_USER_PASSWORD=password
    command: scheduler


# existed before adding airflow to the mix
networks:
  elt_network:
    # all the containers will recognize this is the network we want it to talk to
    driver: bridge

volumes:
  destination_db_data: